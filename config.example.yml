# Bryti configuration
# Copy to data/config.yml and fill in your values.
# ${VAR} references are substituted from environment variables.

agent:
  name: "Bryti"

  # Behavioral core. Memory, tool listings, extensions, and projections are
  # injected automatically. Personal details are best shared in conversation;
  # the agent stores them in core memory across sessions.
  system_prompt: |
    You are Bryti, a personal AI assistant. You run on the pi agent framework
    with persistent memory and tool-calling capabilities. You are concise,
    practical, and honest about what you can and cannot do.

    ## Your memory
    Your core memory (shown below) persists across conversations. Update it when
    you learn something worth keeping: user preferences, facts, ongoing projects,
    recurring topics. Do this proactively without telling the user unless asked.

    Archival memory is for details that don't need to be always visible but should
    be searchable later.

    ## Projection memory
    Projections are your forward-looking memory. Store anything about the
    future: appointments, deadlines, plans, reminders, commitments. Your
    current projections are shown below; use `projection_list` to see more.

    Guidelines:
    - Store ALL items, even far-future ones. If unsure about timing, use
      resolution "month" or "someday"
    - Connect new information to existing projections when you see a link
    - When the user postpones a discussion, create a separate projection for
      each distinct topic being deferred
    - ALWAYS populate the context field with keywords to search archival
      memory with and a brief description of why this matters
    - Before creating a projection, search archival memory first to find
      existing relevant context. Note key terms in the context field so you
      can find them again at activation time
    - If you just discussed something worth projecting, archive the key
      points first, then create the projection referencing what you archived
    - When a projection activates, search archival memory for related context
      before responding. Projections are the "what" and "when"; archival
      memory holds the "why"

    ## What you cannot do
    - You cannot modify your own source code or core configuration
    - You cannot access the internet directly. Use worker_dispatch for any
      web research.

  # Primary model. Format: provider/model-id
  model: "anthropic/claude-sonnet-4-6"

  # Tried in order when primary model fails
  fallback_models:
    - "opencode/minimax-m2.5-free"
    - "opencode/kimi-k2.5-free"

  # IANA timezone
  timezone: "Europe/Amsterdam"

  # Model for the background reflection pass (defaults to primary model)
  # reflection_model: "opencode/minimax-m2.5-free"

# ---------------------------------------------------------------------------
# Channels (enable at least one)
# ---------------------------------------------------------------------------

telegram:
  token: ${TELEGRAM_BOT_TOKEN}
  allowed_users: []  # Telegram user IDs

# WhatsApp via baileys (QR code auth on first run)
# whatsapp:
#   enabled: true
#   allowed_users: []  # international format without + (e.g., 31612345678)

# ---------------------------------------------------------------------------
# Model providers
#
# context_window and max_tokens are optional (defaults: 200K / 32K).
# Only list the models you actually use. See README for more provider examples.
# ---------------------------------------------------------------------------

models:
  providers:
    # Anthropic OAuth (Claude Pro/Max subscription, no API key).
    # One-time: install pi CLI and run `pi login anthropic`.
    - name: anthropic
      base_url: ""
      api: anthropic-messages
      api_key: ""
      models:
        - id: "claude-sonnet-4-6"
          context_window: 200000
          max_tokens: 64000

    # Free models via opencode.ai (no key needed)
    - name: opencode
      base_url: https://opencode.ai/zen/v1
      api: openai-completions
      api_key: "public"
      headers:
        x-opencode-client: "cli"
        x-opencode-project: "global"
        x-opencode-session: "pi-session"
        x-opencode-request: "pi-request"
      models:
        - id: "minimax-m2.5-free"
          name: "MiniMax M2.5 (Free)"
          context_window: 200000
          max_tokens: 32000
          compat:
            maxTokensField: max_tokens
        - id: "kimi-k2.5-free"
          name: "Kimi K2.5 (Free)"
          context_window: 128000
          max_tokens: 32000
          compat:
            maxTokensField: max_tokens

# ---------------------------------------------------------------------------
# Tools
# ---------------------------------------------------------------------------

tools:
  # Web search for workers. Set brave_api_key OR searxng_url.
  web_search:
    # brave_api_key: "${BRAVE_API_KEY}"  # https://api.search.brave.com/
    searxng_url: "https://searx.be"

  workers:
    max_concurrent: 3
    # model: "opencode/minimax-m2.5-free"  # default model for workers

# ---------------------------------------------------------------------------
# Integrations (optional, values become env vars for extensions)
#
# integrations.<name>.<key> → env var NAME_KEY (uppercased)
# Example: integrations.hedgedoc.url → HEDGEDOC_URL
# ---------------------------------------------------------------------------

# integrations:
#   hedgedoc:
#     url: "http://hedgedoc:3000"
#     public_url: "https://docs.example.com"

# ---------------------------------------------------------------------------
# Optional sections (uncomment to customize)
# ---------------------------------------------------------------------------

# cron:
#   - schedule: "0 8 * * *"
#     message: "Good morning! Brief me on anything I need to know today."

# trust:
#   approved_tools: [shell_exec, http_request]

# active_hours:
#   start: "08:00"
#   end: "23:00"
